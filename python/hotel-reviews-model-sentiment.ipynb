{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import save_model\n",
    "import csv \n",
    "import pandas as pd\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows  100000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('hotel-reviews.csv', nrows=100000)\n",
    "df['REVIEW_TEXT'] = df['REVIEW_TEXT'].astype(str)\n",
    "print('Number of rows ', df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment distribution\n",
      "0    56627\n",
      "1    43373\n",
      "Name: REVIEW_SENTIMENT, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Sentiment distribution')\n",
    "print(df[df.columns[1]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df['REVIEW_TEXT']\n",
    "Y_train = df['REVIEW_SENTIMENT']\n",
    "\n",
    "data_text = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process(txt):\n",
    "    out = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "    out = out.split()\n",
    "    out = [word.lower() for word in out]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(thresh = 5):\n",
    "    count  = dict()\n",
    "    idx = 1\n",
    "    word_index = dict()\n",
    "    for txt in data_text:\n",
    "        words = process(txt)\n",
    "        for word in words:\n",
    "            if word in count.keys():\n",
    "                count[word] += 1\n",
    "            else:\n",
    "                count[word]  = 1\n",
    "    most_counts = [word for word in count.keys() if count[word]>=thresh]\n",
    "    for word in most_counts:\n",
    "        word_index[word] = idx\n",
    "        idx+=1\n",
    "    return word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the dictionary  7688\n"
     ]
    }
   ],
   "source": [
    "num_words = None\n",
    "word_index = tokenize()\n",
    "num_words = len(word_index)\n",
    "print('length of the dictionary ',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statement with the largest number of words  399\n"
     ]
    }
   ],
   "source": [
    "def getMax(data):\n",
    "    max_tokens = 0 \n",
    "    for txt in data:\n",
    "        if max_tokens < len(txt.split()):\n",
    "            max_tokens = len(txt.split())\n",
    "    return max_tokens\n",
    "max_tokens = getMax(X_train)\n",
    "print('statement with the largest number of words ', max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data):\n",
    "    tokens = []\n",
    "    counter = 0;\n",
    "    for txt in data:\n",
    "        counter = counter + 1\n",
    "        words = process(txt)\n",
    "        seq = [0] * max_tokens\n",
    "        i = 0 \n",
    "        for word in words:\n",
    "            start = max_tokens-len(words)\n",
    "            if word.lower() in word_index.keys():\n",
    "                seq[i+start] = word_index[word]\n",
    "            i+=1\n",
    "        tokens.append(seq)  \n",
    "        if (counter % 10000 == 0):\n",
    "            print('Rows processed ', counter)\n",
    "    print('Complete ', counter)\n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete  1\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    8 107 792]]\n"
     ]
    }
   ],
   "source": [
    "print(create_sequences(['Bed was comfy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows processed  10000\n",
      "Rows processed  20000\n",
      "Rows processed  30000\n",
      "Rows processed  40000\n",
      "Rows processed  50000\n",
      "Rows processed  60000\n",
      "Rows processed  70000\n",
      "Rows processed  80000\n",
      "Rows processed  90000\n",
      "Rows processed  100000\n",
      "Complete  100000\n"
     ]
    }
   ],
   "source": [
    "X_train_tokens = create_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 13:56:40.976882 140568097670976 deprecation.py:506] From /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0819 13:56:41.013174 140568097670976 deprecation.py:506] From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0819 13:56:41.951501 140568097670976 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_size = 8\n",
    "model.add(Embedding(input_dim=num_words + 1,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer_embedding'))\n",
    "\n",
    "model.add(GRU(units=16, name = \"gru_1\",return_sequences=True))\n",
    "model.add(GRU(units=8, name = \"gru_2\" ,return_sequences=True))\n",
    "model.add(GRU(units=4, name= \"gru_3\"))\n",
    "model.add(Dense(1, activation='sigmoid',name=\"dense_1\"))\n",
    "optimizer = Adam(lr=0.01)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/5\n",
      "80000/80000 [==============================] - 630s 8ms/sample - loss: 0.2050 - acc: 0.9222 - val_loss: 0.1558 - val_acc: 0.9459\n",
      "Epoch 2/5\n",
      "80000/80000 [==============================] - 665s 8ms/sample - loss: 0.1364 - acc: 0.9531 - val_loss: 0.1528 - val_acc: 0.9472\n",
      "Epoch 3/5\n",
      "80000/80000 [==============================] - 606s 8ms/sample - loss: 0.1169 - acc: 0.9607 - val_loss: 0.1511 - val_acc: 0.9472\n",
      "Epoch 4/5\n",
      "80000/80000 [==============================] - 572s 7ms/sample - loss: 0.1062 - acc: 0.9649 - val_loss: 0.1536 - val_acc: 0.9457\n",
      "Epoch 5/5\n",
      "80000/80000 [==============================] - 564s 7ms/sample - loss: 0.0951 - acc: 0.9691 - val_loss: 0.1720 - val_acc: 0.9388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd8228a9a90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(X_train_tokens), np.array(Y_train),\n",
    "          validation_split=0.2, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete  3\n",
      "\n",
      " prediction for \n",
      " [0.00231358 0.9839129  0.00533617]\n"
     ]
    }
   ],
   "source": [
    "txt = [\"Rooms are tastefully decorated to a high standard and the beds are very comfortable\",\"Only 2 lifts and 1 was not operating broken\",\"The bathroom was nice and spacious\"]\n",
    "pred = model.predict(create_sequences(txt))\n",
    "print('\\n prediction for \\n',pred[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(\n",
    "    model,\n",
    "    \"hotel-reviews-model.h5\",\n",
    "    overwrite=True,\n",
    "    include_optimizer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 399, 8)            61512     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 399, 16)           1200      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 399, 8)            600       \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 63,473\n",
      "Trainable params: 63,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(file):\n",
    "    with open(file, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for key in word_index.keys():\n",
    "            writer.writerow([key,word_index[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv('dict.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
